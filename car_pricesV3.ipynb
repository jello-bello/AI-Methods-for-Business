{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car prices dataset assignment\n",
    "\n",
    "*aangeven wat de Research Question is en de Sub Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import * \n",
    "import os\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "from IPython.core.display import HTML \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read data from path\n",
    "\n",
    "An extra column is loaded due to having data but no column name. \n",
    "This is because of an error of data in the dataset and will be adjusted after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# List all the directories and files in the parent directory of 'dataset'\n",
    "dir_list = os.listdir(os.path.join(cwd, 'dataset'))\n",
    "\n",
    "car_prices_data_path = os.path.join( \"dataset\", \"car_prices.csv\")\n",
    "\n",
    "# Read csv file and name column to make sure the 17th column is included\n",
    "df = pd.read_csv(car_prices_data_path, header=None, names=[f'col{i}' for i in range(1, 18)])\n",
    "\n",
    "# Give the first row of this column a value\n",
    "df.iloc[0, 16] = 'extra'\n",
    "\n",
    "# Now, use the first row as column names\n",
    "df.columns = df.iloc[0]\n",
    "\n",
    "# Drop the first row since it's now redundant as column names\n",
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Examine and Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unalligned data is alligned into the correct columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with data in the 'extra' column\n",
    "df_extra = df[df['extra'].notnull()]\n",
    "\n",
    "# Include columns starting from column 5 onwards\n",
    "df_extra = df_extra.iloc[:, 5:]\n",
    "\n",
    "# Replace column names with the name of the column to the right\n",
    "df_extra.columns = df_extra.columns.to_series().shift(+1).fillna('body')\n",
    "\n",
    "# Iterate over the rows of df_extra and update values in df\n",
    "for index, row in df_extra.iterrows():\n",
    "    for column_name in df.columns:\n",
    "        # Update values in df only for shared rows and columns\n",
    "        if column_name in df_extra.columns and index in df.index:\n",
    "            df.at[index, column_name] = row[column_name]\n",
    "\n",
    "# drop the 'extra' column from the dataframe\n",
    "df = df.drop('extra', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to appropriate data types\n",
    "df['year'] = df['year'].astype('int')\n",
    "df['odometer'] = df['odometer'].astype('float')\n",
    "df['mmr'] = df['mmr'].astype('float')\n",
    "df['sellingprice'] = df['sellingprice'].astype('float')\n",
    "df['condition'] = df['condition'].astype('float')\n",
    "df['saledate'] = pd.to_datetime(df['saledate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate shape\n",
    "print(\"There are {} rows and {} columns in the dataset\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "\n",
    "# get statistics for numeric columns\n",
    "nan_counts_numeric = numeric_columns.isnull().sum()\n",
    "unique_counts_numeric = numeric_columns.nunique()\n",
    "mode_counts_numeric = numeric_columns.mode().iloc[0]\n",
    "max_values_numeric = numeric_columns.max()\n",
    "min_values_numeric = numeric_columns.min()\n",
    "percentage_nan_numeric = numeric_columns.isnull().mean() * 100\n",
    "mean_values_numeric = numeric_columns.mean()\n",
    "median_values_numeric = numeric_columns.median()\n",
    "variance_values_numeric = numeric_columns.var()\n",
    "std_dev_values_numeric = numeric_columns.std()\n",
    "skewness_values_numeric = numeric_columns.skew()\n",
    "kurtosis_values_numeric = numeric_columns.kurt()\n",
    "\n",
    "# create a new dataframe with the statistics for numeric columns\n",
    "numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_numeric,\n",
    "                                    'Unique Count': unique_counts_numeric,\n",
    "                                    'Mode Count': mode_counts_numeric,\n",
    "                                    'Max Value': max_values_numeric,\n",
    "                                    'Min Value': min_values_numeric,\n",
    "                                    'Percentage Nan': percentage_nan_numeric,\n",
    "                                    'Mean Value': mean_values_numeric,\n",
    "                                    'Median Value': median_values_numeric,\n",
    "                                    'Variance Value': variance_values_numeric,\n",
    "                                    'Standard Deviation Value': std_dev_values_numeric,\n",
    "                                    'Skewness Value': skewness_values_numeric,\n",
    "                                    'Kurtosis Value': kurtosis_values_numeric})\n",
    "\n",
    "# Print the numeric statistics dataframe\n",
    "print(tabulate(numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non numeric statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(exclude=['number'])\n",
    "\n",
    "# get statistics for non-numeric columns\n",
    "unique_counts_non_numeric = non_numeric_columns.nunique()\n",
    "mode_counts_non_numeric = non_numeric_columns.mode().iloc[0]\n",
    "nan_counts_non_numeric = non_numeric_columns.isnull().sum()\n",
    "percentage_nan_non_numeric = non_numeric_columns.isnull().mean() * 100\n",
    "\n",
    "# create a new dataframe with the statistics for non-numeric columns\n",
    "non_numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_non_numeric,\n",
    "                                       'Unique Count': unique_counts_non_numeric,\n",
    "                                       'Mode Count': mode_counts_non_numeric,\n",
    "                                       'Percentage Nan': percentage_nan_non_numeric})\n",
    "\n",
    "# print the non-numeric statistics dataframe\n",
    "print(tabulate(non_numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('model')\n",
    "modes = grouped['transmission'].apply(lambda x: stats.mode(x)[0][0])\n",
    "df['transmission'].fillna(df['model'].map(modes), inplace=True)\n",
    "\n",
    "# Calculate the mode of the 'transmission' column\n",
    "mode = df['transmission'].mode()[0]\n",
    "\n",
    "# Fill the remaining NaN values in the 'transmission' column with the calculated mode\n",
    "df['transmission'].fillna(mode, inplace=True)\n",
    "\n",
    "# Calculate the mode of the 'transmission' column\n",
    "mode = df['transmission'].mode()[0]\n",
    "\n",
    "# Drop all rows with NaN values in the other rows\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if NaN values are filtered out and shape of remaining dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(exclude=['number'])\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "\n",
    "# get statistics for numeric columns\n",
    "nan_counts_numeric = numeric_columns.isnull().sum()\n",
    "unique_counts_numeric = numeric_columns.nunique()\n",
    "percentage_nan_numeric = numeric_columns.isnull().mean() * 100\n",
    "\n",
    "\n",
    "# create a new dataframe with the statistics for numeric columns\n",
    "numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_numeric,\n",
    "                                    'Unique Count': unique_counts_numeric,\n",
    "                                    'Percentage Nan': percentage_nan_numeric})\n",
    "\n",
    "\n",
    "# print the numeric statistics dataframe\n",
    "print(tabulate(numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n",
    "\n",
    "# get statistics for non-numeric columns\n",
    "unique_counts_non_numeric = non_numeric_columns.nunique()\n",
    "nan_counts_non_numeric = non_numeric_columns.isnull().sum()\n",
    "percentage_nan_non_numeric = non_numeric_columns.isnull().mean() * 100\n",
    "\n",
    "# create a new dataframe with the statistics for non-numeric columns\n",
    "non_numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_non_numeric,\n",
    "                                       'Unique Count': unique_counts_non_numeric,\n",
    "                                       'Percentage Nan': percentage_nan_non_numeric})\n",
    "\n",
    "# print the non-numeric statistics dataframe\n",
    "print(tabulate(non_numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n",
    "\n",
    "# Calculate shape\n",
    "print(\"There are {} rows and {} columns in the dataset\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the predicted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for the 'sellingprice' column\n",
    "sellingprice_stats = df['sellingprice'].describe()\n",
    "\n",
    "# Convert the series to a DataFrame for tabulation\n",
    "sellingprice_stats_df = pd.DataFrame({'Selling Price Stats': sellingprice_stats})\n",
    "\n",
    "# Print the summary statistics using tabulate\n",
    "print(tabulate(sellingprice_stats_df, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Correlation and descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Spectral')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inncluding the categorical features to check the correlation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "df2 = df.copy()\n",
    "\n",
    "# Factorize categorical columns to convert categorical data into numerical format\n",
    "for column in df2.select_dtypes(exclude=[np.number]).columns:\n",
    "     df2[column], labels = pd.factorize(df[column])\n",
    "   \n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df2.corr()\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.clustermap(corr_matrix, annot=True, cmap='Spectral', fmt=\".2f\", vmin=-0.5, vmax=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correaltion matrix above the following explanatory features are selected: \n",
    "year, condition, odometer, color and interior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are outliers in the predicted column using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "Q1 = df['sellingprice'].quantile(0.25)\n",
    "Q3 = df['sellingprice'].quantile(0.75)\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Print relevant values for debugging\n",
    "print(f'- Q1: {Q1} | Q3: {Q3} | IQR: {IQR}')\n",
    "print(f'- Lower Bound: {lower_bound} | Upper Bound: {upper_bound}')\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['sellingprice'] < lower_bound) | (df['sellingprice'] > upper_bound)]\n",
    "\n",
    "# Print the number of outliers\n",
    "num_outliers = len(outliers)\n",
    "print(f'- Number of outliers: {num_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows before removing outliers\n",
    "num_rows_before = len(df)\n",
    "\n",
    "# Remove outliers from the 'sellingprice' column including the sellingprice = 1 \n",
    "df = df[(df['sellingprice'] >= lower_bound) & (df['sellingprice'] <= upper_bound) & (df['sellingprice'] != 1)]\n",
    "\n",
    "# Get the number of outliers removed\n",
    "num_outliers_removed = num_rows_before - len(df)\n",
    "print(f'Number of outliers removed: {num_outliers_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are outliers in the odometer column using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "Q1 = df['odometer'].quantile(0.25)\n",
    "Q3 = df['odometer'].quantile(0.75)\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Print relevant values for debugging\n",
    "print(f'- Q1: {Q1} | Q3: {Q3} | IQR: {IQR}')\n",
    "print(f'- Lower Bound: {lower_bound} | Upper Bound: {upper_bound}')\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['odometer'] < lower_bound) | (df['odometer'] > upper_bound)]\n",
    "\n",
    "# Print the number of outliers\n",
    "num_outliers = len(outliers)\n",
    "print(f'- Number of outliers: {num_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows before removing outliers\n",
    "num_rows_before = len(df)\n",
    "\n",
    "# Remove outliers from the 'sellingprice' column including the sellingprice = 1 \n",
    "df = df[(df['odometer'] >= lower_bound) & (df['odometer'] <= upper_bound)]\n",
    "\n",
    "# Get the number of outliers removed\n",
    "num_outliers_removed = num_rows_before - len(df)\n",
    "print(f'Number of outliers removed: {num_outliers_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are outliers in the year column using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "Q1 = df['year'].quantile(0.25)\n",
    "Q3 = df['year'].quantile(0.75)\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Print relevant values for debugging\n",
    "print(f'- Q1: {Q1} | Q3: {Q3} | IQR: {IQR}')\n",
    "print(f'- Lower Bound: {lower_bound} | Upper Bound: {upper_bound}')\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['year'] < lower_bound) | (df['year'] > upper_bound)]\n",
    "\n",
    "# Print the number of outliers\n",
    "num_outliers = len(outliers)\n",
    "print(f'- Number of outliers: {num_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows before removing outliers\n",
    "num_rows_before = len(df)\n",
    "\n",
    "# Remove outliers from the 'sellingprice' column including the sellingprice = 1 \n",
    "df = df[(df['year'] >= lower_bound) & (df['year'] <= upper_bound)]\n",
    "\n",
    "# Get the number of outliers removed\n",
    "num_outliers_removed = num_rows_before - len(df)\n",
    "print(f'Number of outliers removed: {num_outliers_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a 'ID' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keeping the columns which are needed \n",
    "df = df[['year', 'condition', 'odometer', 'color', 'interior', 'sellingprice']]\n",
    "#Add an 'id' column with values starting from 1\n",
    "df['ID']=range(1, len(df) +1) \n",
    "\n",
    "#Moving the 'id' column to the front\n",
    "df.insert(0, 'ID', df.pop('ID'))\n",
    "\n",
    "#Display the updated datframe \n",
    "print(tabulate(df.head(), headers='keys', tablefmt='fancy_grid'))\n",
    "print(tabulate(df.tail(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the unique values of the explanatory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate({\n",
    "    'Year': df['year'].unique(),\n",
    "    'Condition': df['condition'].unique(),\n",
    "    'Odometer': df['odometer'].unique(),\n",
    "    'Color': df['color'].unique(),\n",
    "    'Interior': df['interior'].unique(),\n",
    " }, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the value counts to identify the categories for the column \"colors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(df['color'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the categories for the column \"color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize colors\n",
    "def categorize_color(color):\n",
    "     \n",
    "    if 'black' in color:\n",
    "        return 'black'\n",
    "    elif 'white' in color:\n",
    "        return 'white'\n",
    "    elif 'silver' in color:\n",
    "        return 'silver'\n",
    "    elif 'gray' in color:\n",
    "        return 'gray'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Apply the function to create a new 'color_category' column\n",
    "df['color_cat'] = df['color'].apply(categorize_color)\n",
    "\n",
    "# Display the count of unique values in 'color_category'\n",
    "print(tabulate(df['color_cat'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the value counts to identify the categories for the column \"colors\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(df['interior'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating categories for column 'interior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize interior\n",
    "def categorize_interior(interior):\n",
    "     \n",
    "    if 'black' in interior:\n",
    "        return 'black'\n",
    "    # elif 'beige' in interior:\n",
    "    #     return 'beige'\n",
    "    elif 'gray' in interior:\n",
    "        return 'gray'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Apply the function to create a new 'color_category' column\n",
    "df['interior_cat'] = df['interior'].apply(categorize_interior)\n",
    "\n",
    "# Display the count of unique values in 'color_category'\n",
    "print(tabulate(df['interior_cat'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the color and interior column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['interior', 'color'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Distribution visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the current distribution of sellingprice after cleaning the data\n",
    "- And normalizing the data in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply square root transformation to 'sellingprice'\n",
    "df['sqrt_sellingprice'] = np.sqrt(df['sellingprice'])\n",
    "\n",
    "# Set a smaller figure size\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Set font size for better readability\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "# Create a histogram of the 'sellingprice' column\n",
    "plt.subplot(1, 2, 1)\n",
    "histplot = sns.histplot(df['sellingprice'], bins=20, kde=True, color='slateblue')\n",
    "\n",
    "# Add frequency counts to each bin\n",
    "for rect in histplot.patches:\n",
    "    height = rect.get_height()\n",
    "    width = rect.get_width()\n",
    "    x = rect.get_x()\n",
    "    y = rect.get_y()\n",
    "\n",
    "    # Add the text annotation\n",
    "    histplot.text(x + width / 2, y + height / 1.5 , f'{int(height)}', ha='center', va='center_baseline')\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Distribution of Selling Prices')\n",
    "plt.xlabel('Selling Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Create a histogram of the 'sellingprice' column\n",
    "plt.subplot(1, 2, 2)\n",
    "histplot = sns.histplot(df['sqrt_sellingprice'], bins=20, kde=True, color='slateblue')\n",
    "\n",
    "# Add frequency counts to each bin\n",
    "for rect in histplot.patches:\n",
    "    height = rect.get_height()\n",
    "    width = rect.get_width()\n",
    "    x = rect.get_x()\n",
    "    y = rect.get_y()\n",
    "\n",
    "    # Add the text annotation\n",
    "    histplot.text(x + width / 2, y + height / 1.9 , f'{int(height)}', ha='center', va='center_baseline')\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Distribution of Normalized Selling Prices')\n",
    "plt.xlabel('Selling Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definine the function generate_freq_table to visualize the categorical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the target variable and calculate the size of each group\n",
    "def generate_freq_table(df, variable = ['sellingprice']):\n",
    "    dfs = []\n",
    "    for i in variable:\n",
    "        df_count = (\n",
    "            df.groupby(i, observed=False)\n",
    "            .size()\n",
    "            .reset_index(name='N')\n",
    "            .assign(var = i)\n",
    "            .rename(columns={i: 'category'})\n",
    "        )\n",
    "        # Append the result to the list of DataFrames\n",
    "        dfs.append(df_count)\n",
    "        # Concatenate all DataFrames in the list\n",
    "        res = pd.concat(dfs)\n",
    "        # Convert the 'category' column to string type\n",
    "        res['category'] = res['category'].astype(str)\n",
    "    # Return the final concatenated DataFrame\n",
    "    return res\n",
    "\n",
    "generate_freq_table(df, ['interior_cat', 'color_cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to create the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_freq_plot(freq_table):\n",
    "    return (\n",
    "        ggplot(freq_table, aes(x='var', y='N', fill='category')) +\n",
    "        geom_col(stat='identity', position='dodge')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the interior categories plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_freq_plot(generate_freq_table(df, ['interior_cat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the color categories plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_freq_plot(generate_freq_table(df, ['color_cat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the plot for year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the frequency plot of year with different colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "histplot = sns.histplot(data=df, x='year', hue='year', bins=15, palette='viridis', kde=False)\n",
    "plt.title('Frequency Plot of Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the plot for condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the frequency plot of condition with different colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "histplot = sns.histplot(data=df, x='condition', hue='condition', bins=41, palette='magma')\n",
    "plt.title('Frequency Plot of Condition')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "#Modify legend \n",
    "plt.legend(title='Condition', loc='upper left', ncol=2, labels= (df['condition'].unique()))\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating the plot for odometer\n",
    "- And normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply square root transformation to 'age'\n",
    "df['sqrt_odometer'] = np.sqrt(df['odometer'])\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(16, 6))  # Increased the figure width to accommodate both plots\n",
    "\n",
    "# Create the first histogram of the 'odometer' column\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['odometer'], bins=20, color='lightseagreen')\n",
    "plt.title('Histogram of Odometer')\n",
    "plt.xlabel('Odometer')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Create the second histogram of a different column (replace 'another_column' with the actual column name)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['sqrt_odometer'], bins=20, color='lightseagreen')  # Replace 'another_column' with the actual column name\n",
    "plt.title('Histogram of Normalized Odometer')\n",
    "plt.xlabel('Odometer logged')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Trainclasssplitter class with 3 def fucntions:\n",
    "- Innitialize instances\n",
    "- Statistics calculator\n",
    "- Train, test and validation splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['color_cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestSplitter(object):\n",
    "    '''Class to perform the split of the data into train, test, and validation.\n",
    "    '''\n",
    "    def __init__(self, train_frac=0.8, validation_frac=0.2, seed=1234, num_bins=5):\n",
    "        self.train_frac = train_frac\n",
    "        self.validation_frac = validation_frac\n",
    "        self.seed = seed\n",
    "        self.num_bins = num_bins\n",
    "        self.total_n_sellingprice_bin = {}\n",
    "    \n",
    "    def calculate_statistics(self):\n",
    "        statistics = {}\n",
    "        for i in ['train_set', 'test_set', 'validation_set']:\n",
    "            split_stats = {}\n",
    "            \n",
    "            # Create bins for sellingprice\n",
    "            bins = np.linspace(getattr(self, i)['sellingprice'].min(), getattr(self, i)['sellingprice'].max(), self.num_bins + 1)\n",
    "            getattr(self, i)['sellingprice_bin'] = pd.cut(getattr(self, i)['sellingprice'], bins=bins)\n",
    "            \n",
    "            target_count = getattr(self, i).groupby('sellingprice_bin').size().reset_index()\n",
    "            \n",
    "            for _, row in target_count.iterrows():\n",
    "                bin_label = str(row['sellingprice_bin'])\n",
    "                bin_count = row[0]\n",
    "                percentage_key = f'percentage_total_sellingprice_bin_{bin_label}'\n",
    "                split_stats[f'N_sellingprice_bin_{bin_label}'] = bin_count\n",
    "                split_stats[percentage_key] = bin_count / self.total_n_sellingprice_bin.get(bin_label, 1) * 100\n",
    "\n",
    "            statistics[i] = split_stats\n",
    "\n",
    "        self.split_statistics = statistics\n",
    "\n",
    "    def split_train_test(self, df):\n",
    "        print(\"Generating the train/validation/test splits...\")\n",
    "        \n",
    "        # Create bins for sellingprice in the entire dataset\n",
    "        bins = np.linspace(df['sellingprice'].min(), df['sellingprice'].max(), self.num_bins + 1)\n",
    "        df['sellingprice_bin'] = pd.cut(df['sellingprice'], bins=bins)\n",
    "        \n",
    "        for bin_label in df['sellingprice_bin'].unique():\n",
    "            bin_count = df.loc[lambda x: x.sellingprice_bin == bin_label].shape[0]\n",
    "            self.total_n_sellingprice_bin[str(bin_label)] = bin_count\n",
    "\n",
    "        self.train_set = df.sample(frac=self.train_frac, random_state=self.seed)\n",
    "        self.test_set = df.loc[lambda x: ~x.ID.isin(self.train_set.ID)].reset_index(drop=True)\n",
    "        self.validation_set = self.train_set.sample(frac=self.validation_frac).reset_index(drop=True)\n",
    "        self.train_set = self.train_set.loc[lambda x: ~x.ID.isin(self.validation_set.ID)].reset_index(drop=True)\n",
    "        print(\"Calculating the statistics...\")\n",
    "        self.calculate_statistics()\n",
    "        print(\"Split completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fitting_splits object that will hold the train, validation, and test data\n",
    "fitting_splits = TrainTestSplitter()\n",
    "fitting_splits.split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_splits.test_set.shape\n",
    "fitting_splits.split_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 def functions:\n",
    "- Dummificator\n",
    "- Scaler\n",
    "- A function that uses the other 2 def functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify(df, one_hot_encoder):\n",
    "    vars_to_encode = ['interior_cat', 'color_cat']\n",
    "    df_to_encode = df[vars_to_encode]\n",
    "    if not one_hot_encoder:\n",
    "        one_hot_encoder = OneHotEncoder()\n",
    "        df_encoded = one_hot_encoder.fit_transform(df_to_encode).toarray()\n",
    "    else:\n",
    "        df_encoded = one_hot_encoder.transform(df_to_encode).toarray()\n",
    "    df_encoded = pd.DataFrame(df_encoded, columns=one_hot_encoder.get_feature_names_out())\n",
    "    # add the encoded columns and drop the original columns\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "    df = df.drop(vars_to_encode, axis=1)\n",
    "    return df, one_hot_encoder\n",
    "\n",
    "def scale(df, standard_scaler, cols_to_scale):\n",
    "    if not standard_scaler:\n",
    "        standard_scaler = StandardScaler()\n",
    "        df[cols_to_scale] = standard_scaler.fit_transform(df[cols_to_scale])\n",
    "    else:\n",
    "        df[cols_to_scale] = standard_scaler.transform(df[cols_to_scale])\n",
    "    return df, standard_scaler\n",
    "\n",
    "def prepare_data(df, one_hot_encoder=None, standard_scaler=None, cols_to_scale=None):\n",
    "    df = df.reset_index(drop=True)\n",
    "    df, one_hot_encoder = dummify(df, one_hot_encoder)\n",
    "    \n",
    "    # Identify columns to scale (numerical features)\n",
    "    if cols_to_scale is None:\n",
    "        cols_to_scale = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Identify columns to exclude from scaling (one-hot encoded columns)\n",
    "    one_hot_columns = one_hot_encoder.get_feature_names_out() if one_hot_encoder else []\n",
    "    cols_to_exclude = df.columns[df.columns.isin(one_hot_columns)]\n",
    "    \n",
    "    # Remove one-hot encoded columns from the list of columns to scale\n",
    "    cols_to_scale = list(set(cols_to_scale) - set(cols_to_exclude))\n",
    "    \n",
    "    df, standard_scaler = scale(df, standard_scaler, cols_to_scale)\n",
    "    return df, one_hot_encoder, standard_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we prepare all the data we use below\n",
    "X_train, one_hot_encoder, standard_scaler = prepare_data(fitting_splits.train_set)\n",
    "X_train = X_train.drop([\"ID\", 'odometer', 'sellingprice', 'sqrt_sellingprice', 'sellingprice_bin'],axis=1)  \n",
    "y_train = fitting_splits.train_set[\"sqrt_sellingprice\"]\n",
    "\n",
    "X_validation = prepare_data(fitting_splits.validation_set, one_hot_encoder, standard_scaler)[0]\n",
    "X_validation = X_validation.drop([\"ID\", 'odometer', 'sellingprice', 'sqrt_sellingprice', 'sellingprice_bin'],axis=1)\n",
    "y_validation = fitting_splits.validation_set[\"sqrt_sellingprice\"]\n",
    "\n",
    "X_test = prepare_data(fitting_splits.test_set, one_hot_encoder, standard_scaler)[0].drop(['sellingprice_bin',\"ID\", 'odometer', 'sellingprice', 'sqrt_sellingprice'],axis=1)\n",
    "y_test = fitting_splits.test_set[\"sqrt_sellingprice\"]\n",
    "\n",
    "X_train_validation = pd.concat([X_train, X_validation])\n",
    "y_train_validation = pd.concat([y_train, y_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_validation.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_validation.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with your GridSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": [x for x in range(10, 200, 10)],\n",
    "    \"max_depth\": [x for x in range(5, 21, 5)]\n",
    "}\n",
    "\n",
    "sklearn_grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid=param_grid, n_jobs=4, scoring='neg_mean_squared_error')\n",
    "_ = sklearn_grid_search_rf.fit(X_train_validation, y_train_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding to create binary columns for each category and indicates the presence of a category with a 1 or 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding with a prefix for color_cat\n",
    "# df = pd.get_dummies(df, columns=['color_cat'], prefix='color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding with a prefix for interior_Cat\n",
    "# df = pd.get_dummies(df, columns=['interior_cat'], prefix='interior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Checking how the dataframe looks after encoding\n",
    "# print(tabulate(df.head(), headers='keys', tablefmt='fancy_grid'))\n",
    "# print(f'The DataFrame has {df.shape[0]} rows and {df.shape[1]} columns.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('teaching')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1e146a432e2971ead6ca2adfccd361e7256016801521af6894ffdef8a064a536"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
